{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "import FrozenAgent\n",
    "import random\n",
    "import time\n",
    "\n",
    "semilla=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"definimos el entorno:\")\n",
    "#@title Importamos el lago helado\n",
    "name = 'FrozenLake-v1'\n",
    "env4 = gym.make(name, is_slippery=False, map_name=\"4x4\", render_mode=\"ansi\") # No resbaladizo para entender mejor los resultados.\n",
    "env8 = gym.make(name, is_slippery=False, map_name=\"8x8\", render_mode=\"ansi\") # No resbaladizo para entender mejor los resultados.\n",
    "\n",
    "#env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=n_episodes)\n",
    "\n",
    "ts1=[]\n",
    "ts2=[]\n",
    "ts3=[]\n",
    "\n",
    "def setSemilla(semilla):\n",
    "    random.seed(semilla)\n",
    "    np.random.seed(semilla)\n",
    "\n",
    "def train_agent(agent, env, num_episodes=5000, decay=False, semilla=1):\n",
    "    agent.initAgent()\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        state, info = env.reset(seed=semilla)\n",
    "        done = False\n",
    "    \n",
    "        start_time=time.time_ns()\n",
    "        #inicializo el episodio\n",
    "        agent.initEpisode()\n",
    "    \n",
    "        # play one episode\n",
    "        while not done:\n",
    "            if decay:\n",
    "                agent.decay_epsilon()\n",
    "            action = agent.get_action(env, state)\n",
    "            \n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # update the agent\n",
    "            agent.updateStep(state, action, reward, terminated, next_state)\n",
    "            \n",
    "            # update if the environment is done and the current state\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "            \n",
    "        t1 = time.time_ns()\n",
    "\n",
    "        #después de acabar el episodio actualizo la Q y el epsilon\n",
    "        agent.updateEpisode()\n",
    "        \n",
    "        t2 = time.time_ns()\n",
    "        ts1.append((t2-start_time)/1000)\n",
    "        ts2.append((t1-start_time)/1000)\n",
    "        ts3.append((t2-t1)/1000)\n",
    "\n",
    "def plot(agent):\n",
    "  # Creamos una lista de índices para el eje x\n",
    "  indices = list(range(len(agent.list_stats)))\n",
    "  \n",
    "  \n",
    "  # Crear figura con dos subgráficos\n",
    "  fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))\n",
    "\n",
    "  # Primer subplot\n",
    "  ax1.plot(indices, agent.list_stats, label='stats')\n",
    "  ax1.set_title('Proporción de recompensas')\n",
    "  ax1.set_xlabel('Episodio')\n",
    "  ax1.set_ylabel('Proporción')\n",
    "  ax1.legend()\n",
    "  ax1.grid(True)\n",
    "\n",
    "  # Segundo subplot\n",
    "  ax2.plot(indices, agent.list_episodes, label='episodios')\n",
    "  ax2.set_title('Tamaño de episodios')\n",
    "  ax2.set_xlabel('Episodio')\n",
    "  ax2.set_ylabel('Tamaño')\n",
    "  ax2.legend()\n",
    "  ax2.grid(True)\n",
    "\n",
    "  # Ajustar diseño y mostrar gráfico\n",
    "  plt.tight_layout()\n",
    "  plt.show()  \n",
    "\n",
    "\n",
    "\n",
    "#inicializo los numeros aleatorios\n",
    "setSemilla(semilla)\n",
    "\n",
    "# hyperparameters\n",
    "n_episodes = 50000\n",
    "start_epsilon = 0.1\n",
    "discount_factor = 0.99\n",
    "\n",
    "#agent4 = FrozenAgent.FrozenAgentMC_On_All(\n",
    "#agent4 = FrozenAgent.FrozenAgentMC_On_First(\n",
    "#agent4 = FrozenAgent.FrozenAgentGreedy(\n",
    "#agent4 = FrozenAgent.FrozenAgentMC_Off_Q(\n",
    "#agent4 = FrozenAgent.FrozenAgentSARSA(\n",
    "agent4 = FrozenAgent.FrozenAgentQ_Learning(\n",
    "    env=env4,\n",
    "    epsilon=start_epsilon,\n",
    "    discount_factor=discount_factor,\n",
    "    alpha=0.1\n",
    ")\n",
    "\n",
    "train_agent(agent4, env4, num_episodes=n_episodes, decay=False, semilla=semilla)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "agent4b = FrozenAgent.FrozenAgentMC_On_First(\n",
    "    env=env4,\n",
    "    epsilon=start_epsilon,\n",
    "    discount_factor=discount_factor,\n",
    ")\n",
    "\n",
    "train_agent(agent4b, env4, num_episodes=n_episodes, decay=False, semilla=semilla)\n",
    "\n",
    "np.set_printoptions(precision=6)\n",
    "diff=np.array(agent4b.Q-agent4.Q)\n",
    "print(type(diff))\n",
    "print(f\"Valores Q para cada estado:\\n {agent4.Q}\")\n",
    "print(f\"Valores Qb para cada estado:\\n {agent4b.Q}\")\n",
    "print(f\"Valores Q-Qb para cada estado:\\n {diff}\")\n",
    "exit()\n",
    "'''\n",
    "plot(agent4)\n",
    "print(f\"Máxima proporcion: {agent4.list_stats[-1]}\")\n",
    "\n",
    "LEFT, DOWN, RIGHT, UP = 0,1,2,3\n",
    "print(\"Valores Q para cada estado:\\n\", agent4.Q)\n",
    "print(\"Valores pi para cada estado:\\n\", agent4.policy)\n",
    "\n",
    "LEFT, DOWN, RIGHT, UP = 0,1,2,3\n",
    "pi, actions = agent4.pi_star_from_Q(env4, agent4.Q)\n",
    "\n",
    "print(\"Política óptima obtenida\\n\", pi, f\"\\n Acciones {actions} \\n Para el siguiente grid\\n\", env4.render())\n",
    "print()\n",
    "\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(8, 6))\n",
    "\n",
    "# Primer subplot\n",
    "ax1.plot(np.arange(len(ts1)),ts1, label='ts1')\n",
    "ax1.grid(True)\n",
    "ax2.plot(np.arange(len(ts2)),ts2, label='ts2')\n",
    "ax2.grid(True)\n",
    "ax3.plot(np.arange(len(ts3)),ts3, label='ts3')\n",
    "ax3.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()  \n",
    "\n",
    "\n",
    "exit()\n",
    "#ahora entrenamos con el mapa de 8x8\n",
    "\n",
    "#inicializo los numeros aleatorios\n",
    "setSemilla(semilla)\n",
    "\n",
    "# hyperparameters\n",
    "n_episodes = 50000\n",
    "start_epsilon = 0.4\n",
    "discount_factor = 1.0\n",
    "\n",
    "agent8 = FrozenAgent.FrozenAgentGreedy(\n",
    "    env=env8,\n",
    "    epsilon=start_epsilon,\n",
    "    discount_factor=discount_factor,\n",
    ")\n",
    "\n",
    "train_agent(agent8, env8, num_episodes=n_episodes, decay=True, semilla=semilla)\n",
    "\n",
    "plot(agent8)\n",
    "print(f\"Máxima proporcion: {agent8.list_stats[-1]}\")\n",
    "     \n",
    "LEFT, DOWN, RIGHT, UP = 0,1,2,3\n",
    "print(\"Valores Q para cada estado:\\n\", agent8.Q)\n",
    "\n",
    "LEFT, DOWN, RIGHT, UP = 0,1,2,3\n",
    "pi, actions = agent8.pi_star_from_Q(env8, agent8.Q)\n",
    "\n",
    "print(\"Política óptima obtenida\\n\", pi, f\"\\n Acciones {actions} \\n Para el siguiente grid\\n\", env8.render())\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
