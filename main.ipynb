{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/> Main](https://colab.research.google.com/github/Andrew-mcArty/RL_AAPGAB/blob/main/main.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "El propósito de este documento es servir de nexo entre las diferentes partes de la práctica 2\n",
    "\n",
    "* A) Conocer el entorno Gymnasium\n",
    "  * Construir un agente de acuerdo a las indicaciones de Gymnasium para que aprenda de acuerdo al algortimo explicado en el notebook del tutor\n",
    "  * Añadir gráfica *f(t) = len(episodio)*\n",
    "* B) Estudio de algunos algoritmos básicos\n",
    "  * Métodos tabulares: Monte Carlo (on-policy y off-policy), Diferencias Temporales (SARSA y Q-Learning)\n",
    "  * Control con Aproximaciones: SARSA semi-gradiente, Deep Q-Learning\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura principal del agente\n",
    "\n",
    "Todos los agentes definidos en esta práctica tienen la siguiente estructura:\n",
    "* *\\_\\_init\\_\\_(self, env, epsilon, discount_factor)*: Inicializa la clase con los parámetros comunes: epsilon y fator de descuento\n",
    "* *initAgent(self)*: inicializa el agente con la configuración inicial parametros del epsilon greedy y su decaimiento\n",
    "* *initEpisode(self)*: Inicializa el agente con un nuevo episodio\n",
    "* *get_action(self, env, state)*: Conocido el estado actual, devuelve una acción\n",
    "* *updateStep(self, state, action, reward, terminated, next_state)*: Actualiza el estado interno del agente a partir de la información obtenida tras ejecutar la acción a nivel de step (Gymnasium)\n",
    "* *updateEpisode(self)*: Actualiza el agente a nivel de episodio\n",
    "* *decay_epsilon(self)*: implementa el mecanismo de decaimiento del parámetro epsilon\n",
    "* *pi_star_from_Q(self, Q)*: obtiene la política óptima a partir de Q\n",
    "* ...otros métodos internos...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura principal de entrenamiento de un agente\n",
    "\n",
    "Todos los estudios tienen en común el siguiente algoritmo que nos permite entrenar cada uno de los agentes implementados:\n",
    "\n",
    "* inicializar el agente: agent.initAgent()\n",
    "* repetir hasta *num_episodes*:\n",
    "  * resetear el entorno: env.reset(seed)\n",
    "  * indicar al agente que comienza un nuevo episodio: agent.iniEpisode()\n",
    "  * mientras no terminado o truncado:\n",
    "    * opcionalmente decaer epsilon: agent.decay_epsilon()\n",
    "    * obtener una nueva acción: agent.get_action(env,state)\n",
    "    * ejecutar la acción: env.step(action)\n",
    "    * actualizar el estado del agente a nivel de step: agent.updateStepstate,action,reward,termintated,truncated,next_state)\n",
    "    * establecer el nuevo estado\n",
    "  * actualizar el estado del agente a nivel de episodio: agent.updateEpisode()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura de los estudios comparativos\n",
    "\n",
    "En todos los estudios se seguira esta secuencia de pasos:\n",
    "* Inicializar el entorno. El entorno elegido es Frozen Lake (4x4 y 8x8) para los métodos tabulares y Mountain Car para control aproximado\n",
    "* Definir dos agentes:\n",
    "  * Para entornos tabulares se ha definido las siguientes clases, todas son de control y estiman Q salvo que se indique lo contrario:\n",
    "    * FrozenAgentGreedy: agente que implementa Monte Carlo Todas las visitas según cuaderno del tutor\n",
    "    * FrozenAgentMC_On_First: agente que implementa Monte Carlos On Policy Primera visita\n",
    "    * FrozenAgentMC_On_All: agente que implementa Monte Carlos On Policy Todas las visitas\n",
    "    * FrozenAgentMC_Off_Pi: agente que implementa Monte Carlo Off Policy para estimar Pi \n",
    "    * FrozenAgentMC_Off_Q: agente que implementa Monte Carlo Off Policy\n",
    "    * FrozenAgentSARSA: agente que implementa Diferencias Temporales SARSA\n",
    "    * FrozenAgentQ_Learning: agente que implementa Diferencias Temporales Q-Learning\n",
    "* Entrenar cada agente unos 50.000 episodios\n",
    "* Mostrar resultados:\n",
    "  * Máxima proporción: suma de ganancias/numero episodios\n",
    "  * Valores Q obtenidos\n",
    "  * Política óptima\n",
    "* Analizar y conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diferentes estudios\n",
    "\n",
    "A continuación se muestra cada uno de los diferentes estudios\n",
    "\n",
    "* A) Conocer el entorno Gymnasium\n",
    "  * Construir un agente de acuerdo a las indicaciones de Gymnasium para que aprenda de acuerdo al algortimo explicado en el notebook del tutor\n",
    "  * Añadir gráfica *f(t) = len(episodio)*\n",
    "  \n",
    "    [<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/> Primer Agente](https://colab.research.google.com/github/Andrew-mcArty/RL_AAPGAB/blob/main/primer_agente.ipynb)\n",
    "* B) Estudio de algunos algoritmos básicos\n",
    "  * Métodos tabulares: Monte Carlo (on-policy y off-policy), Diferencias Temporales (SARSA y Q-Learning)\n",
    "    * Estudio Monte Carlo.\n",
    "      Este cuaderno explora las diferencias entre algunas de las diferentes alternativas de Monte Carlo: on y off-policy, first y all visits\n",
    "      \n",
    "      [<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/> Estudio Monte Carlo](https://colab.research.google.com/github/Andrew-mcArty/RL_AAPGAB/blob/main/estudio_montecarlo.ipynb)\n",
    "\n",
    "    * Estudio Diferencias Temporales.\n",
    "      Este cuaderno explora la diferencia entre SARSA y Q-Learning para resolver el problema de Frozen Lake\n",
    "      \n",
    "      [<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/> Estudio Diferencias Temporales](https://colab.research.google.com/github/Andrew-mcArty/RL_AAPGAB/blob/main/estudio_td.ipynb)\n",
    "  * Control con Aproximaciones: SARSA semi-gradiente, Deep Q-Learning\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
